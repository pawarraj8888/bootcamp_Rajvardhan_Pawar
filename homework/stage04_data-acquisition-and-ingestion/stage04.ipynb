{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d322fba-19d9-45dd-a8e8-72745b711df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CONFIG (edit) ====\n",
    "TICKER = \"AAPL\"\n",
    "RAW_DIR = \"data/raw\"\n",
    "\n",
    "# ==== IMPORTS ====\n",
    "import os, re, requests, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "try:\n",
    "    from dotenv import load_dotenv; load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ==== FETCHERS ====\n",
    "def fetch_alpha_vantage_daily(ticker: str) -> pd.DataFrame:\n",
    "    key = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "    if not key:\n",
    "        raise RuntimeError(\"No ALPHAVANTAGE_API_KEY in .env\")\n",
    "    url = \"https://www.alphavantage.co/query\"\n",
    "    params = {\n",
    "        \"function\":\"TIME_SERIES_DAILY_ADJUSTED\",\n",
    "        \"symbol\":ticker,\n",
    "        \"outputsize\":\"full\",\n",
    "        \"apikey\":key,\n",
    "        \"datatype\":\"json\",\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30); r.raise_for_status()\n",
    "    data = r.json()\n",
    "    if \"Note\" in data or \"Information\" in data:\n",
    "        raise RuntimeError(data.get(\"Note\") or data.get(\"Information\"))\n",
    "    ts = data.get(\"Time Series (Daily)\")\n",
    "    if not ts:\n",
    "        raise RuntimeError(f\"Unexpected AV response keys: {list(data)[:6]}\")\n",
    "    df = (pd.DataFrame(ts).T\n",
    "            .rename(columns={\n",
    "                \"1. open\":\"Open\",\"2. high\":\"High\",\"3. low\":\"Low\",\"4. close\":\"Close\",\n",
    "                \"5. adjusted close\":\"Adj Close\",\"6. volume\":\"Volume\"\n",
    "            })\n",
    "            .reset_index().rename(columns={\"index\":\"Date\"}))\n",
    "    return df\n",
    "\n",
    "def fetch_yfinance(ticker: str) -> pd.DataFrame:\n",
    "    import pandas as pd\n",
    "    import yfinance as yf\n",
    "    df = yf.download(\n",
    "        ticker,\n",
    "        period=\"max\",\n",
    "        auto_adjust=False,\n",
    "        progress=False,\n",
    "        group_by=\"column\",   # <- avoids ticker-first MultiIndex in most cases\n",
    "    )\n",
    "    if df is None or df.empty:\n",
    "        raise RuntimeError(\"yfinance returned no data\")\n",
    "\n",
    "    # Flatten any MultiIndex reliably\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # If columns look like (TICKER, Field), keep Field\n",
    "        lvl1 = df.columns.get_level_values(0).unique().tolist()\n",
    "        lvl2 = df.columns.get_level_values(-1).unique().tolist()\n",
    "        if set([\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\"]).issubset(set(lvl2)):\n",
    "            df.columns = df.columns.get_level_values(-1)\n",
    "        else:\n",
    "            # generic flatten: join with space\n",
    "            df.columns = [\" \".join([str(x) for x in tup if x not in (None,\"\")]).strip()\n",
    "                          for tup in df.columns.to_list()]\n",
    "    else:\n",
    "        # some builds prefix columns like \"AAPL Open\"\n",
    "        df.columns = [str(c) for c in df.columns]\n",
    "\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "# ==== STANDARDIZE & VALIDATE (ultra-tolerant) ===\n",
    "def standardize_and_validate(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    import re, numpy as np, pandas as pd\n",
    "\n",
    "    # 1) force string cols & flatten if needed\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [\" \".join([str(x) for x in tup if x not in (None, \"\")]).strip()\n",
    "                      for tup in df.columns.to_list()]\n",
    "    else:\n",
    "        df.columns = [str(c) for c in df.columns]\n",
    "\n",
    "    # 2) tolerant matcher\n",
    "    def find_col(cols, *must_have, exclude=None):\n",
    "        exclude = [e.lower() for e in (exclude or [])]\n",
    "        for c in cols:\n",
    "            lc = c.lower()\n",
    "            if all(m in lc for m in [m.lower() for m in must_have]) and not any(e in lc for e in exclude):\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    # locate candidates\n",
    "    col_date  = find_col(df.columns, \"date\") or find_col(df.columns, \"timestamp\") or find_col(df.columns, \"datetime\")\n",
    "    col_open  = find_col(df.columns, \"open\")\n",
    "    col_high  = find_col(df.columns, \"high\")\n",
    "    col_low   = find_col(df.columns, \"low\")\n",
    "    col_adj   = find_col(df.columns, \"adj\", \"close\")\n",
    "    # key fix: accept things like \"Close AAPL\", but exclude \"Adj Close\"\n",
    "    col_close = find_col(df.columns, \"close\", exclude=[\"adj\"])\n",
    "\n",
    "    # fallback regex for CLOSE (handles weird spacing/prefix/suffix)\n",
    "    if col_close is None:\n",
    "        cands = [c for c in df.columns if re.search(r\"\\bclose\\b\", c, flags=re.I) and \"adj\" not in c.lower()]\n",
    "        if cands:\n",
    "            col_close = cands[0]\n",
    "\n",
    "    col_vol   = find_col(df.columns, \"volume\") or find_col(df.columns, \"vol\")\n",
    "\n",
    "    # 3) rename to canonical\n",
    "    rename = {}\n",
    "    if col_date:  rename[col_date]  = \"Date\"\n",
    "    if col_open:  rename[col_open]  = \"Open\"\n",
    "    if col_high:  rename[col_high]  = \"High\"\n",
    "    if col_low:   rename[col_low]   = \"Low\"\n",
    "    if col_close: rename[col_close] = \"Close\"\n",
    "    if col_adj:   rename[col_adj]   = \"Adj Close\"\n",
    "    if col_vol:   rename[col_vol]   = \"Volume\"\n",
    "    df = df.rename(columns=rename)\n",
    "\n",
    "    # 4) dtypes\n",
    "    if \"Date\" in df.columns:\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if \"Volume\" in df.columns:\n",
    "        v = pd.to_numeric(df[\"Volume\"], errors=\"coerce\")\n",
    "        try: df[\"Volume\"] = v.astype(\"Int64\")\n",
    "        except Exception: df[\"Volume\"] = v\n",
    "\n",
    "    # 5) validate\n",
    "    required = [\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        print(\"DEBUG columns after rename:\", list(df.columns))\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Empty DataFrame after fetch\")\n",
    "    na = df[required].isna().sum()\n",
    "    if na.get(\"Date\",0) > 0 or na.get(\"Close\",0) > 0:\n",
    "        raise ValueError(f\"NAs in critical columns:\\n{na}\")\n",
    "\n",
    "    # 6) order & sort\n",
    "    ordered = [c for c in [\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\"] if c in df.columns]\n",
    "    df = df[ordered + [c for c in df.columns if c not in ordered]].sort_values(\"Date\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==== PIPELINE ====\n",
    "def pull_and_save(ticker: str, raw_dir: str) -> str:\n",
    "    Path(raw_dir).mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        df = fetch_alpha_vantage_daily(ticker); src = \"alphavantage\"\n",
    "    except Exception as e:\n",
    "        print(f\"Alpha Vantage failed → {e}\\nFalling back to yfinance.\")\n",
    "        df = fetch_yfinance(ticker); src = \"yfinance\"\n",
    "\n",
    "    df = standardize_and_validate(df)\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out = Path(raw_dir)/f\"{ticker.upper()}_{src}_{ts}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"✅ Saved {len(df):,} rows → {out}\")\n",
    "    return str(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc0b999d-7318-4561-8501-bd3e5338ea75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha Vantage failed → Thank you for using Alpha Vantage! This is a premium endpoint. You may subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly unlock all premium endpoints\n",
      "Falling back to yfinance.\n",
      "✅ Saved 11,265 rows → data/raw/AAPL_yfinance_20250824_004133.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/raw/AAPL_yfinance_20250824_004133.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = pull_and_save(TICKER, RAW_DIR)\n",
    "csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da18db7-c4b5-44fa-a60e-cb5233f6ef8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7c265a6-207d-4d72-b5ce-95a3f90472b6",
   "metadata": {},
   "source": [
    "STEP 2 : Scrape a Small Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297afad3-3aed-49ba-9f2f-7651c2136284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e99fd-3702-41ae-985e-966e0d7944dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fe-course)",
   "language": "python",
   "name": "fe-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
