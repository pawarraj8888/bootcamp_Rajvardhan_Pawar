{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d322fba-19d9-45dd-a8e8-72745b711df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CONFIG (edit) ====\n",
    "TICKER = \"AAPL\"\n",
    "RAW_DIR = \"data/raw\"\n",
    "\n",
    "# ==== IMPORTS ====\n",
    "import os, re, requests, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "try:\n",
    "    from dotenv import load_dotenv; load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ==== FETCHERS ====\n",
    "def fetch_alpha_vantage_daily(ticker: str) -> pd.DataFrame:\n",
    "    key = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "    if not key:\n",
    "        raise RuntimeError(\"No ALPHAVANTAGE_API_KEY in .env\")\n",
    "    url = \"https://www.alphavantage.co/query\"\n",
    "    params = {\n",
    "        \"function\":\"TIME_SERIES_DAILY_ADJUSTED\",\n",
    "        \"symbol\":ticker,\n",
    "        \"outputsize\":\"full\",\n",
    "        \"apikey\":key,\n",
    "        \"datatype\":\"json\",\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30); r.raise_for_status()\n",
    "    data = r.json()\n",
    "    if \"Note\" in data or \"Information\" in data:\n",
    "        raise RuntimeError(data.get(\"Note\") or data.get(\"Information\"))\n",
    "    ts = data.get(\"Time Series (Daily)\")\n",
    "    if not ts:\n",
    "        raise RuntimeError(f\"Unexpected AV response keys: {list(data)[:6]}\")\n",
    "    df = (pd.DataFrame(ts).T\n",
    "            .rename(columns={\n",
    "                \"1. open\":\"Open\",\"2. high\":\"High\",\"3. low\":\"Low\",\"4. close\":\"Close\",\n",
    "                \"5. adjusted close\":\"Adj Close\",\"6. volume\":\"Volume\"\n",
    "            })\n",
    "            .reset_index().rename(columns={\"index\":\"Date\"}))\n",
    "    return df\n",
    "\n",
    "def fetch_yfinance(ticker: str) -> pd.DataFrame:\n",
    "    import pandas as pd\n",
    "    import yfinance as yf\n",
    "    df = yf.download(\n",
    "        ticker,\n",
    "        period=\"max\",\n",
    "        auto_adjust=False,\n",
    "        progress=False,\n",
    "        group_by=\"column\",   # <- avoids ticker-first MultiIndex in most cases\n",
    "    )\n",
    "    if df is None or df.empty:\n",
    "        raise RuntimeError(\"yfinance returned no data\")\n",
    "\n",
    "    # Flatten any MultiIndex reliably\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # If columns look like (TICKER, Field), keep Field\n",
    "        lvl1 = df.columns.get_level_values(0).unique().tolist()\n",
    "        lvl2 = df.columns.get_level_values(-1).unique().tolist()\n",
    "        if set([\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\"]).issubset(set(lvl2)):\n",
    "            df.columns = df.columns.get_level_values(-1)\n",
    "        else:\n",
    "            # generic flatten: join with space\n",
    "            df.columns = [\" \".join([str(x) for x in tup if x not in (None,\"\")]).strip()\n",
    "                          for tup in df.columns.to_list()]\n",
    "    else:\n",
    "        # some builds prefix columns like \"AAPL Open\"\n",
    "        df.columns = [str(c) for c in df.columns]\n",
    "\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "# ==== STANDARDIZE & VALIDATE (ultra-tolerant) ===\n",
    "def standardize_and_validate(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    import re, numpy as np, pandas as pd\n",
    "\n",
    "    # 1) force string cols & flatten if needed\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [\" \".join([str(x) for x in tup if x not in (None, \"\")]).strip()\n",
    "                      for tup in df.columns.to_list()]\n",
    "    else:\n",
    "        df.columns = [str(c) for c in df.columns]\n",
    "\n",
    "    # 2) tolerant matcher\n",
    "    def find_col(cols, *must_have, exclude=None):\n",
    "        exclude = [e.lower() for e in (exclude or [])]\n",
    "        for c in cols:\n",
    "            lc = c.lower()\n",
    "            if all(m in lc for m in [m.lower() for m in must_have]) and not any(e in lc for e in exclude):\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    # locate candidates\n",
    "    col_date  = find_col(df.columns, \"date\") or find_col(df.columns, \"timestamp\") or find_col(df.columns, \"datetime\")\n",
    "    col_open  = find_col(df.columns, \"open\")\n",
    "    col_high  = find_col(df.columns, \"high\")\n",
    "    col_low   = find_col(df.columns, \"low\")\n",
    "    col_adj   = find_col(df.columns, \"adj\", \"close\")\n",
    "    # key fix: accept things like \"Close AAPL\", but exclude \"Adj Close\"\n",
    "    col_close = find_col(df.columns, \"close\", exclude=[\"adj\"])\n",
    "\n",
    "    # fallback regex for CLOSE (handles weird spacing/prefix/suffix)\n",
    "    if col_close is None:\n",
    "        cands = [c for c in df.columns if re.search(r\"\\bclose\\b\", c, flags=re.I) and \"adj\" not in c.lower()]\n",
    "        if cands:\n",
    "            col_close = cands[0]\n",
    "\n",
    "    col_vol   = find_col(df.columns, \"volume\") or find_col(df.columns, \"vol\")\n",
    "\n",
    "    # 3) rename to canonical\n",
    "    rename = {}\n",
    "    if col_date:  rename[col_date]  = \"Date\"\n",
    "    if col_open:  rename[col_open]  = \"Open\"\n",
    "    if col_high:  rename[col_high]  = \"High\"\n",
    "    if col_low:   rename[col_low]   = \"Low\"\n",
    "    if col_close: rename[col_close] = \"Close\"\n",
    "    if col_adj:   rename[col_adj]   = \"Adj Close\"\n",
    "    if col_vol:   rename[col_vol]   = \"Volume\"\n",
    "    df = df.rename(columns=rename)\n",
    "\n",
    "    # 4) dtypes\n",
    "    if \"Date\" in df.columns:\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if \"Volume\" in df.columns:\n",
    "        v = pd.to_numeric(df[\"Volume\"], errors=\"coerce\")\n",
    "        try: df[\"Volume\"] = v.astype(\"Int64\")\n",
    "        except Exception: df[\"Volume\"] = v\n",
    "\n",
    "    # 5) validate\n",
    "    required = [\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        print(\"DEBUG columns after rename:\", list(df.columns))\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Empty DataFrame after fetch\")\n",
    "    na = df[required].isna().sum()\n",
    "    if na.get(\"Date\",0) > 0 or na.get(\"Close\",0) > 0:\n",
    "        raise ValueError(f\"NAs in critical columns:\\n{na}\")\n",
    "\n",
    "    # 6) order & sort\n",
    "    ordered = [c for c in [\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\"] if c in df.columns]\n",
    "    df = df[ordered + [c for c in df.columns if c not in ordered]].sort_values(\"Date\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==== PIPELINE ====\n",
    "def pull_and_save(ticker: str, raw_dir: str) -> str:\n",
    "    Path(raw_dir).mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        df = fetch_alpha_vantage_daily(ticker); src = \"alphavantage\"\n",
    "    except Exception as e:\n",
    "        print(f\"Alpha Vantage failed → {e}\\nFalling back to yfinance.\")\n",
    "        df = fetch_yfinance(ticker); src = \"yfinance\"\n",
    "\n",
    "    df = standardize_and_validate(df)\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out = Path(raw_dir)/f\"{ticker.upper()}_{src}_{ts}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\" Saved {len(df):,} rows → {out}\")\n",
    "    return str(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc0b999d-7318-4561-8501-bd3e5338ea75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha Vantage failed → Thank you for using Alpha Vantage! This is a premium endpoint. You may subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly unlock all premium endpoints\n",
      "Falling back to yfinance.\n",
      " Saved 11,265 rows → data/raw/AAPL_yfinance_20250824_004640.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/raw/AAPL_yfinance_20250824_004640.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = pull_and_save(TICKER, RAW_DIR)\n",
    "csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da18db7-c4b5-44fa-a60e-cb5233f6ef8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7c265a6-207d-4d72-b5ce-95a3f90472b6",
   "metadata": {},
   "source": [
    "# STEP 2 : Scrape a Small Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "297afad3-3aed-49ba-9f2f-7651c2136284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 503 rows, 8 columns → data/raw/sp500_constituents_20250824_005648.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/raw/sp500_constituents_20250824_005648.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === CONFIG (edit if you want another table) ===\n",
    "URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"  # public, permitted\n",
    "TABLE_ID = \"constituents\"  # this page's main table id\n",
    "RAW_DIR = \"data/raw\"\n",
    "OUT_BASENAME = \"sp500_constituents\"  # used in filename\n",
    "REQUIRED_COLS = [\"Symbol\", \"Security\", \"GICS Sector\", \"GICS Sub-Industry\"]\n",
    "\n",
    "# === IMPORTS ===\n",
    "import os, re, requests, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# === HELPERS ===\n",
    "def _ensure_dir(p): Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _clean_header(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "    # common wiki header cleanups\n",
    "    return (s.replace(\"Ticker symbol\",\"Symbol\")\n",
    "             .replace(\"GICS sector\",\"GICS Sector\")\n",
    "             .replace(\"GICS sub-industry\",\"GICS Sub-Industry\"))\n",
    "\n",
    "def _clean_text(s: str) -> str:\n",
    "    if s is None: return \"\"\n",
    "    s = re.sub(r\"\\[\\d+\\]\", \"\", s)      # remove citation [1], [2], ...\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _maybe_numeric(series: pd.Series) -> pd.Series:\n",
    "    # try coercing strings like \"1,234.56\" or \"12.3%\" to numeric\n",
    "    s = series.astype(str).str.replace(\",\", \"\", regex=False).str.replace(\"%\",\"\", regex=False)\n",
    "    num = pd.to_numeric(s, errors=\"coerce\")\n",
    "    # Only keep numeric if we got at least some non-null numeric values\n",
    "    return num if num.notna().sum() >= max(3, int(0.25*len(num))) else series\n",
    "\n",
    "def _validate_df(df: pd.DataFrame, required_cols=None):\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Parsed table is empty.\")\n",
    "    if required_cols:\n",
    "        missing = [c for c in required_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    # Basic NA + dtype sanity on any numeric-looking columns\n",
    "    numeric_like = [c for c in df.columns if df[c].dtype.kind in \"if\"]\n",
    "    if numeric_like:\n",
    "        na_counts = df[numeric_like].isna().sum()\n",
    "        # allow some NAs but flag if everything is NA in a numeric column\n",
    "        fully_na = [c for c in numeric_like if df[c].notna().sum() == 0]\n",
    "        if fully_na:\n",
    "            raise ValueError(f\"Numeric columns fully NA after parse: {fully_na}\")\n",
    "\n",
    "# === SCRAPER (BeautifulSoup) ===\n",
    "def scrape_table_to_df(url: str, table_id: str | None = None, table_index: int = 0) -> pd.DataFrame:\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (educational project; contact=student@example.com)\"}\n",
    "    r = requests.get(url, headers=headers, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # pick the table\n",
    "    table = None\n",
    "    if table_id:\n",
    "        table = soup.find(\"table\", id=table_id)\n",
    "        if table is None:\n",
    "            raise RuntimeError(f\"No table with id='{table_id}' on page.\")\n",
    "    else:\n",
    "        tables = soup.find_all(\"table\")\n",
    "        if not tables:\n",
    "            raise RuntimeError(\"No <table> elements on page.\")\n",
    "        table = tables[table_index]\n",
    "\n",
    "    # parse header\n",
    "    ths = table.find_all(\"th\")\n",
    "    header_cells = [ _clean_header(_clean_text(th.get_text())) for th in ths ]\n",
    "\n",
    "    # If header row spans multiple lines/sections, fall back to first <tr> th/tds\n",
    "    if not header_cells:\n",
    "        first_tr = table.find(\"tr\")\n",
    "        header_cells = [ _clean_header(_clean_text(x.get_text())) for x in first_tr.find_all([\"th\",\"td\"]) ]\n",
    "\n",
    "    # parse rows\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        tds = tr.find_all(\"td\")\n",
    "        if not tds:  # skip header rows\n",
    "            continue\n",
    "        row = [ _clean_text(td.get_text()) for td in tds ]\n",
    "        rows.append(row)\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"Table has no data rows.\")\n",
    "\n",
    "    # align column count\n",
    "    max_cols = max(len(r) for r in rows)\n",
    "    cols = header_cells[:max_cols] if header_cells else [f\"col_{i}\" for i in range(max_cols)]\n",
    "    rows_padded = [ r + [\"\"]*(max_cols - len(r)) for r in rows ]\n",
    "\n",
    "    df = pd.DataFrame(rows_padded, columns=cols)\n",
    "\n",
    "    # de-dup any duplicate column names that wiki sometimes has\n",
    "    if df.columns.duplicated().any():\n",
    "        new_cols = []\n",
    "        seen = {}\n",
    "        for c in df.columns:\n",
    "            if c not in seen:\n",
    "                seen[c] = 0\n",
    "                new_cols.append(c)\n",
    "            else:\n",
    "                seen[c] += 1\n",
    "                new_cols.append(f\"{c}_{seen[c]}\")\n",
    "        df.columns = new_cols\n",
    "\n",
    "    # Attempt numeric coercion for any columns that look numeric-ish\n",
    "    for c in df.columns:\n",
    "        # if majority of cells look like numbers/percentages, try cast\n",
    "        sample = df[c].astype(str).str.replace(\",\",\"\", regex=False).str.replace(\"%\",\"\", regex=False)\n",
    "        looks_numeric = sample.str.fullmatch(r\"-?\\d+(\\.\\d+)?\").mean() > 0.5\n",
    "        if looks_numeric:\n",
    "            df[c] = _maybe_numeric(df[c])\n",
    "\n",
    "    return df\n",
    "\n",
    "# === PIPELINE ===\n",
    "def scrape_validate_save(url: str, table_id: str | None, out_basename: str,\n",
    "                         raw_dir: str, required_cols=None) -> str:\n",
    "    _ensure_dir(raw_dir)\n",
    "    df = scrape_table_to_df(url, table_id)\n",
    "    # minimal cleanup for this specific table: drop empty columns if any\n",
    "    df = df.loc[:, [c for c in df.columns if c and c.strip()]]\n",
    "    _validate_df(df, required_cols=required_cols)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out = Path(raw_dir)/f\"{out_basename}_{ts}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"✅ Scraped {len(df):,} rows, {df.shape[1]} columns → {out}\")\n",
    "    return str(out)\n",
    "\n",
    "# === RUN ===\n",
    "csv_path_table = scrape_validate_save(URL, TABLE_ID, OUT_BASENAME, RAW_DIR, REQUIRED_COLS)\n",
    "csv_path_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e99fd-3702-41ae-985e-966e0d7944dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51202890-8e5c-4fcd-a864-000e53c4e06d",
   "metadata": {},
   "source": [
    "# Step 3 — Documentation\n",
    "\n",
    "## Data Sources (public & permitted)\n",
    "1) **Price history (primary)**  \n",
    "   - Source: Yahoo Finance via `yfinance` (fallback)  \n",
    "   - URL pattern: https://finance.yahoo.com/quote/{TICKER}/history  \n",
    "   - Chosen Ticker: **AAPL**  \n",
    "   - Notes: Alpha Vantage (requests) attempted first; endpoint required premium → fallback engaged.\n",
    "\n",
    "2) **Small table (scrape)**  \n",
    "   - Source: Wikipedia — S&P 500 constituents  \n",
    "   - URL: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies  \n",
    "   - Table ID: `constituents`\n",
    "\n",
    "## API / Scrape Parameters\n",
    "**Price Pull**\n",
    "- `period=\"max\"`, `auto_adjust=False`, `group_by=\"column\"`, `progress=False`\n",
    "- Canonical columns expected: `Date, Open, High, Low, Close, Adj Close, Volume`\n",
    "\n",
    "**Scrape**\n",
    "- HTTP method: `GET` with simple user-agent\n",
    "- Parser: `BeautifulSoup(\"html.parser\")`\n",
    "- Table selection: by `id=\"constituents\"` (falls back to first table if a different URL is used)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4094f2-59ad-4b3a-a1ab-9081cf764574",
   "metadata": {},
   "source": [
    "## Validation Logic\n",
    "\n",
    "**Price Data (OHLCV)**\n",
    "- Columns required: `Date, Open, High, Low, Close, Volume`\n",
    "- Column discovery is tolerant (handles variants like `\"Close AAPL\"` and MultiIndex).\n",
    "- Dtype coercion:\n",
    "  - `Date` → datetime (`errors=\"coerce\"`)\n",
    "  - `Open, High, Low, Close, Adj Close` → numeric floats\n",
    "  - `Volume` → integer-like (`Int64`) where possible\n",
    "- Fail-fast checks:\n",
    "  - Missing any required column → **error**\n",
    "  - Empty DataFrame after fetch → **error**\n",
    "  - Any NA in `Date` or `Close` → **error**\n",
    "- Tidying: sort by `Date`, canonical column order.\n",
    "\n",
    "**Scraped Table**\n",
    "- Table must not be empty.\n",
    "- Required text columns (for S&P 500 page): `Symbol, Security, GICS Sector, GICS Sub-Industry`\n",
    "- Numeric-looking columns are coerced; if a column ends up fully NA after coercion → **error**.\n",
    "- Drop obviously blank columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2d087b-16f6-483b-8251-96e68bc51fcf",
   "metadata": {},
   "source": [
    "## Secrets & Compliance\n",
    "\n",
    "- **`.env` is local only** and contains: `ALPHAVANTAGE_API_KEY=...`\n",
    "- **Committed**: `.env.example` (template only)\n",
    "- **Git ignore**: `.env` is listed in `.gitignore`\n",
    "\n",
    "**Paths**\n",
    "- Raw API CSV → `data/raw/<TICKER>_<source>_<timestamp>.csv`\n",
    "- Raw scraped CSV → `data/raw/sp500_constituents_<timestamp>.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ad14087-9fe5-4f66-9154-df9dd8f30b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ git ls-files .env\n",
      "<no output>\n",
      "$ git ls-files .env.example\n",
      "<no output>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "❌ .env.example is not tracked!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 2) Ensure .env.example IS tracked\u001b[39;00m\n\u001b[32m     17\u001b[39m tracked_example = run(\u001b[33m\"\u001b[39m\u001b[33mgit ls-files .env.example\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m tracked_example.endswith(\u001b[33m\"\u001b[39m\u001b[33m.env.example\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33m❌ .env.example is not tracked!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ .env is NOT committed, .env.example IS committed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: ❌ .env.example is not tracked!"
     ]
    }
   ],
   "source": [
    "# This cell verifies we didn't commit secrets.\n",
    "\n",
    "\n",
    "import subprocess, shlex\n",
    "\n",
    "def run(cmd):\n",
    "    print(f\"$ {cmd}\")\n",
    "    out = subprocess.check_output(shlex.split(cmd)).decode().strip()\n",
    "    print(out or \"<no output>\")\n",
    "    return out\n",
    "\n",
    "# 1) Ensure .env is not tracked\n",
    "tracked = run(\"git ls-files .env\")\n",
    "assert tracked == \"\", \"❌ .env appears to be tracked!\"\n",
    "\n",
    "# 2) Ensure .env.example IS tracked\n",
    "tracked_example = run(\"git ls-files .env.example\")\n",
    "assert tracked_example.endswith(\".env.example\"), \"❌ .env.example is not tracked!\"\n",
    "\n",
    "print(\"✅ .env is NOT committed, .env.example IS committed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9801c38-f103-4cdc-b22e-5f1b74c3627a",
   "metadata": {},
   "source": [
    "## Assumptions & Risks\n",
    "\n",
    "**Assumptions**\n",
    "- Yahoo Finance historical data is sufficiently accurate for coursework backtesting.\n",
    "- Wikipedia table structure (`id=\"constituents\"`) remains stable during the assignment window.\n",
    "- Network access is allowed for educational use (rate limits acceptable).\n",
    "\n",
    "**Risks**\n",
    "- **Schema drift**: Yahoo/Wikipedia can change column names or table structure → scraper/standardizer may break.\n",
    "- **Delistings & symbol changes**: Ticker histories or S&P 500 membership can change; reproducibility depends on timestamped raw snapshots.\n",
    "- **Partial outages / throttling**: API or site may throttle; fallback to cached raw CSVs if re-runs fail.\n",
    "- **Timezones**: `Date` normalized by parsing rules; daily bars are treated as naive dates (documented).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e270f-e690-43b0-a34e-11e940d9970a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fe-course)",
   "language": "python",
   "name": "fe-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
